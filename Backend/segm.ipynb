{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "def aumentarDados(imagem):\n",
    "    seq = iaa.Sequential([\n",
    "        #iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        #iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        #iaa.Multiply((0.8, 1.2)),\n",
    "        iaa.GammaContrast((0.2, 0.9)),\n",
    "        #iaa.Affine(scale=(0.8, 1.2))\n",
    "    ])\n",
    "    return seq.augment_image(imagem)\n",
    "\n",
    "img = cv2.imread(\"imagem.png\")\n",
    "img_cinza = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "imagens_aumentadas = []\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    img_aumentada = aumentarDados(img_cinza)\n",
    "    imagens_aumentadas.append(img_aumentada)\n",
    "    nome_arquivo = \"teste/imagem_aumentada_\" + str(i+1) + \".png\"\n",
    "    cv2.imwrite(nome_arquivo, img_aumentada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "p1 = 'TB_Chest_Radiography_Database/Tuberculosis/Tuberculosis-21.png'\n",
    "\n",
    "def equalize_histogram(img):\n",
    "    img_eq = cv2.equalizeHist(img)\n",
    "    return img_eq\n",
    "\n",
    "\n",
    "img = cv2.imread(p1, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "img_eq = equalize_histogram(img)\n",
    "clahe=cv2.createCLAHE(clipLimit=20)\n",
    "eg=clahe.apply(img_eq)\n",
    "eqhist_images=np.concatenate((img,eg),axis=1)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(eqhist_images,cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ic = cv2.bitwise_not(img)\n",
    "eqhist_images=np.concatenate((img,ic),axis=1)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(eqhist_images,cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_path = 'TB_Chest_Radiography_Database/Tuberculosis/Tuberculosis-1.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "gray_img = gray_img/255.0\n",
    "im_power_law_transformation = cv2.pow(gray_img, 0.6)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.hstack((gray_img*255, im_power_law_transformation*255)),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_eq = cv2.equalizeHist(img)\n",
    "eg= cv2.GaussianBlur(eg, (5,5), 0)\n",
    "_, thresh = cv2.threshold(eg, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "dist_transform = cv2.distanceTransform(thresh, cv2.DIST_L2, 5)\n",
    "_, sure_fg = cv2.threshold(dist_transform, 0.5*dist_transform.max(), 255, 0)\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(thresh, sure_fg)\n",
    "\n",
    "# Threshold eg image to obtain a binary image\n",
    "_, binary_eg = cv2.threshold(eg, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "_, markers = cv2.connectedComponents(binary_eg)\n",
    "markers = markers + 1\n",
    "markers[unknown==255] = 0\n",
    "\n",
    "# Convert eg to 3-channel image\n",
    "eg = cv2.cvtColor(eg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "markers = cv2.watershed(eg, markers)\n",
    "eg[markers == -1] = 255\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(eg,cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = 'TB_Chest_Radiography_Database/Tuberculosis'\n",
    "output_dir = 'turberculoseC'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "image_files = os.listdir(image_dir)\n",
    "\n",
    "for imag_file in image_files:\n",
    "    imag_P = os.path.join(image_dir,imag_file)\n",
    "    \n",
    "\n",
    "    if img is None:\n",
    "        print(\"Erro\")\n",
    "        continue\n",
    "\n",
    "    #img_Red = cv2.resize(img, (0, 0), fx=0.6, fy=0.6)\n",
    "    #img_g = cv2.GaussianBlur(img_Red, (5, 5), 0)\n",
    "    #gray = cv2.cvtColor(img_g, cv2.COLOR_BGR2GRAY)\n",
    "    #thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    #kernel = np.ones((3, 3), np.uint8)\n",
    "    #opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "    #contours, hierarchy = cv2.findContours(opening, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #cv2.drawContours(img, contours, -1, (0, 255, 0), 2)\n",
    "    img = cv2.imread(imag_P, cv2.IMREAD_GRAYSCALE)\n",
    "    eg = cv2.equalizeHist(img)\n",
    "    clahe=cv2.createCLAHE(clipLimit=5)\n",
    "    eg=clahe.apply(eg)\n",
    "    eg= cv2.GaussianBlur(eg, (5,5), 0)\n",
    "    _, thresh = cv2.threshold(eg, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    dist_transform = cv2.distanceTransform(thresh, cv2.DIST_L2, 5)\n",
    "    _, sure_fg = cv2.threshold(dist_transform, 0.5*dist_transform.max(), 255, 0)\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(thresh, sure_fg)\n",
    "\n",
    "    # Threshold eg image to obtain a binary image\n",
    "    _, binary_eg = cv2.threshold(eg, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "    _, markers = cv2.connectedComponents(binary_eg)\n",
    "    markers = markers + 1\n",
    "    markers[unknown==255] = 0\n",
    "\n",
    "    # Convert eg to 3-channel image\n",
    "    eg = cv2.cvtColor(eg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    markers = cv2.watershed(eg, markers)\n",
    "    eg[markers == -1] = 255\n",
    "\n",
    "    output_file = os.path.join(output_dir, f'resized_{imag_file}')\n",
    "    cv2.imwrite(output_file, eg)\n",
    "\n",
    "\n",
    "    # img_Red=cv2.resize(img,(0,0), fx=0.6, fy=0.6)\n",
    "    # #plt.imshow(img_Red)\n",
    "    # img_g= cv2.GaussianBlur(img_Red,(5,5),0)\n",
    "    # #plt.imshow(img_g)\n",
    "    # gray = cv2.cvtColor(img_g, cv2.COLOR_BGR2GRAY)\n",
    "    # ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    # kernel = np.ones((3,3),np.uint8)\n",
    "    # erosion = cv2.erode(thresh,kernel,iterations = 1)\n",
    "    # dilation = cv2.dilate(erosion,kernel,iterations = 1)\n",
    "    # contours, hierarchy = cv2.findContours(dilation, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # cv2.drawContours(img, contours, -1, (0,255,0), 2)\n",
    "    # output_file = os.path.join(output_dir, f'resized_{imag_file}')\n",
    "    # cv2.imwrite(output_file,dilation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Definindo diretórios\n",
    "p1 = 'turberculoseC'\n",
    "p2 = 'turberculoseN'\n",
    "train_dir = 'train2'\n",
    "validation_dir = 'validation2'\n",
    "\n",
    "# Criando as pastas train e validation\n",
    "if not os.path.exists(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "\n",
    "if not os.path.exists(validation_dir):\n",
    "    os.makedirs(validation_dir)\n",
    "\n",
    "# Definindo o tamanho do conjunto de treino e validação\n",
    "train_size = 0.7\n",
    "validation_size = 0.3\n",
    "\n",
    "# Mesclando as imagens das pastas p1 e p2 de forma aleatória\n",
    "images = []\n",
    "for img_name in os.listdir(p1):\n",
    "    images.append((os.path.join(p1, img_name), 1))\n",
    "for img_name in os.listdir(p2):\n",
    "    images.append((os.path.join(p2, img_name), 0))\n",
    "random.shuffle(images)\n",
    "\n",
    "# Copiando as imagens mescladas para as pastas train e validation\n",
    "for i, (image_path, label) in enumerate(images):\n",
    "    if i < len(images) * train_size:\n",
    "        dst_dir = os.path.join(train_dir, str(label))\n",
    "    else:\n",
    "        dst_dir = os.path.join(validation_dir, str(label))\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    shutil.copy(image_path, os.path.join(dst_dir, os.path.basename(image_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo diretórios\n",
    "train_dir = 'train'\n",
    "validation_dir = 'validation'\n",
    "\n",
    "# Definindo número de classes e tamanho das imagens\n",
    "num_classes = 2\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Definindo o tamanho do batch de imagens a ser utilizado no treinamento\n",
    "batch_size = 30\n",
    "\n",
    "# Definindo geradores de imagens para treinamento e validação com aumento de dados\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.4,\n",
    "                                   zoom_range=0.4,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Criando geradores de imagens a partir dos diretórios\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(img_width, img_height),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(validation_dir,\n",
    "                                                       target_size=(img_width, img_height),\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       class_mode='categorical')\n",
    "\n",
    "# Definindo a arquitetura da ResNet50 com pesos pré-treinados no ImageNet\n",
    "resnet_model = ResNet50(include_top=False, weights='imagenet',\n",
    "                        input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Congelando as camadas convolucionais da ResNet50\n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Adicionando camadas densas para classificação\n",
    "x = resnet_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Definindo o modelo final\n",
    "model = tf.keras.models.Model(inputs=resnet_model.input, outputs=output)\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=train_generator.n // batch_size,\n",
    "                    epochs=25,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_generator.n // batch_size)\n",
    "\n",
    "# Avaliando o modelo na validação\n",
    "scores = model.evaluate(validation_generator)\n",
    "print(f'Acurácia na validação: {scores[1]*100}%')\n",
    "\n",
    "# Salvando o modelo treinado\n",
    "model.save('tb_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"TB_Chest_Radiography_Database/Tuberculosis/Tuberculosis-1.png\")\n",
    "\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)[1]\n",
    "\n",
    "\n",
    "model = cv2.dnn.readNetFromTensorflow(\"deeplab_v3_plus.pb\")\n",
    "blob = cv2.dnn.blobFromImage(img, swapRB=True, crop=False)\n",
    "model.setInput(blob)\n",
    "output = model.forward()\n",
    "output = output[0, :, :, :]\n",
    "classes = cv2.imread(\"labels.png\")\n",
    "classes = cv2.resize(classes, (output.shape[1], output.shape[0]))\n",
    "classes = cv2.cvtColor(classes, cv2.COLOR_BGR2GRAY)\n",
    "classes = np.array(classes)\n",
    "classes = classes[:, :, np.newaxis]\n",
    "colors = np.array([[0, 0, 0], [128, 0, 0], [0, 128, 0]])\n",
    "mask = np.argmax(output, axis=2)\n",
    "mask = colors[mask]\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_RGB2BGR)\n",
    "mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "mask = cv2.addWeighted(img, 0.5, mask, 0.5, 0)\n",
    "cv2.imshow(\"dd\", mask)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m proporcao_treinamento \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[39m# Dividir as bases de dados e gerar as imagens aumentadas\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m dividir_bases_dados(pasta_tuberculose, pasta_sem_tuberculose, proporcao_treinamento)\n",
      "Cell \u001b[1;32mIn[70], line 67\u001b[0m, in \u001b[0;36mdividir_bases_dados\u001b[1;34m(pasta_tuberculose, pasta_sem_tuberculose, proporcao_treinamento)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m# Gerar imagens aumentadas para o conjunto de treinamento com tuberculose\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m img_tuberculose \u001b[39min\u001b[39;00m imagens_tuberculose_treinamento:\n\u001b[1;32m---> 67\u001b[0m     aumentarDados(img_tuberculose, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     69\u001b[0m \u001b[39m# Gerar imagens aumentadas para o conjunto de treinamento sem tuberculose\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m img_sem_tuberculose \u001b[39min\u001b[39;00m imagens_sem_tuberculose_treinamento:\n",
      "Cell \u001b[1;32mIn[70], line 31\u001b[0m, in \u001b[0;36maumentarDados\u001b[1;34m(imagem, tem_tuberculose)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m# Salvar a imagem\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mif\u001b[39;00m tem_tuberculose:\n\u001b[1;32m---> 31\u001b[0m     cv2\u001b[39m.\u001b[39;49mimwrite(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain/imagem_aumentada_tuberculose_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m\"\u001b[39;49m, img_eq)\n\u001b[0;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     cv2\u001b[39m.\u001b[39mimwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain/imagem_aumentada_sem_tuberculose_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m, )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from imgaug import augmenters as iaa\n",
    "import random\n",
    "\n",
    "def aumentarDados(imagem, tem_tuberculose):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        iaa.GammaContrast((0.8, 1.2)),\n",
    "        iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25),\n",
    "        iaa.Affine(scale=(0.8, 1.2))\n",
    "    ])\n",
    "\n",
    " \n",
    "    imagens_aumentadas = seq.augment_images([imagem] * 5)  # Gerar 10 imagens aumentadas\n",
    "\n",
    "    # Salvar as imagens resultantes\n",
    "    for i, img in enumerate(imagens_aumentadas):\n",
    "        # Aplicar Equalização e Limiarização\n",
    "        img_eq = cv2.equalizeHist(img)\n",
    "     \n",
    "     \n",
    "        # Salvar a imagem\n",
    "        if tem_tuberculose:\n",
    "            cv2.imwrite(f\"train/imagem_aumentada_tuberculose_{i}.png\", img_eq)\n",
    "        else:\n",
    "            cv2.imwrite(f\"train/imagem_aumentada_sem_tuberculose_{i}.png\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo gasto na versão sequencial: 352.0161859989166\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "def aumentarDados(imagem):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        iaa.GammaContrast((0.8, 1.2)),\n",
    "        iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25),\n",
    "        iaa.Affine(scale=(0.8, 1.2))\n",
    "    ])\n",
    "\n",
    "    imagens_aumentadas = seq.augment_images([imagem] * 5)  # Gerar 5 imagens aumentadas\n",
    "\n",
    "    return imagens_aumentadas\n",
    "\n",
    "def aplicar_aumento_dados(pasta_train):\n",
    "    pasta_destino_aumentadas_0 = os.path.join(pasta_train, \"0\")\n",
    "    pasta_destino_aumentadas_1 = os.path.join(pasta_train, \"1\")\n",
    "\n",
    "    # Criar os diretórios de destino se não existirem\n",
    "    if not os.path.exists(pasta_destino_aumentadas_0):\n",
    "        os.makedirs(pasta_destino_aumentadas_0)\n",
    "    if not os.path.exists(pasta_destino_aumentadas_1):\n",
    "        os.makedirs(pasta_destino_aumentadas_1)\n",
    "\n",
    "    # Percorrer a pasta \"0\" e aplicar aumento de dados nas imagens\n",
    "    pasta_origem_0 = os.path.join(pasta_train, \"0\")\n",
    "    for arquivo in os.listdir(pasta_origem_0):\n",
    "        if arquivo.endswith(\".png\"):\n",
    "            caminho_origem = os.path.join(pasta_origem_0, arquivo)\n",
    "            imagem = cv2.imread(caminho_origem)\n",
    "            imagens_aumentadas = aumentarDados(imagem)\n",
    "            for i, img_aumentada in enumerate(imagens_aumentadas):\n",
    "                nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "                caminho_destino = os.path.join(pasta_destino_aumentadas_0, nome_arquivo)\n",
    "                cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "    # Percorrer a pasta \"1\" e aplicar aumento de dados nas imagens\n",
    "    pasta_origem_1 = os.path.join(pasta_train, \"1\")\n",
    "    for arquivo in os.listdir(pasta_origem_1):\n",
    "        if arquivo.endswith(\".png\"):\n",
    "            caminho_origem = os.path.join(pasta_origem_1, arquivo)\n",
    "            imagem = cv2.imread(caminho_origem)\n",
    "            imagens_aumentadas = aumentarDados(imagem)\n",
    "            for i, img_aumentada in enumerate(imagens_aumentadas):\n",
    "                nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "                caminho_destino = os.path.join(pasta_destino_aumentadas_1, nome_arquivo)\n",
    "                cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "\n",
    "\n",
    "pasta_train = \"train2\"\n",
    "start_time = time.time()\n",
    "aplicar_aumento_dados(pasta_train)\n",
    "sequencial_time = time.time() - start_time\n",
    "\n",
    "print(\"Tempo gasto na versão sequencial:\", sequencial_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17640 images belonging to 2 classes.\n",
      "Found 1260 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "551/551 [==============================] - 875s 2s/step - loss: 0.5394 - accuracy: 0.8403 - val_loss: 1.0881 - val_accuracy: 0.8349\n",
      "Epoch 2/30\n",
      "551/551 [==============================] - 865s 2s/step - loss: 0.5547 - accuracy: 0.8001 - val_loss: 0.4700 - val_accuracy: 0.8381\n",
      "Epoch 3/30\n",
      "551/551 [==============================] - 903s 2s/step - loss: 0.4623 - accuracy: 0.8318 - val_loss: 0.4498 - val_accuracy: 0.8357\n",
      "Epoch 4/30\n",
      "551/551 [==============================] - 913s 2s/step - loss: 0.4537 - accuracy: 0.8320 - val_loss: 0.4446 - val_accuracy: 0.8373\n",
      "Epoch 5/30\n",
      "551/551 [==============================] - 854s 2s/step - loss: 0.4529 - accuracy: 0.8320 - val_loss: 0.4469 - val_accuracy: 0.8357\n",
      "Epoch 6/30\n",
      "551/551 [==============================] - 855s 2s/step - loss: 0.4525 - accuracy: 0.8322 - val_loss: 0.4455 - val_accuracy: 0.8365\n",
      "Epoch 7/30\n",
      "551/551 [==============================] - 858s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4441 - val_accuracy: 0.8373\n",
      "Epoch 8/30\n",
      "551/551 [==============================] - 875s 2s/step - loss: 0.4525 - accuracy: 0.8321 - val_loss: 0.4480 - val_accuracy: 0.8349\n",
      "Epoch 9/30\n",
      "551/551 [==============================] - 1429s 3s/step - loss: 0.4528 - accuracy: 0.8320 - val_loss: 0.4454 - val_accuracy: 0.8365\n",
      "Epoch 10/30\n",
      "551/551 [==============================] - 835s 2s/step - loss: 0.4528 - accuracy: 0.8320 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 11/30\n",
      "551/551 [==============================] - 836s 2s/step - loss: 0.4526 - accuracy: 0.8321 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 12/30\n",
      "551/551 [==============================] - 834s 2s/step - loss: 0.4531 - accuracy: 0.8318 - val_loss: 0.4442 - val_accuracy: 0.8373\n",
      "Epoch 13/30\n",
      "551/551 [==============================] - 834s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4429 - val_accuracy: 0.8381\n",
      "Epoch 14/30\n",
      "551/551 [==============================] - 834s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4468 - val_accuracy: 0.8357\n",
      "Epoch 15/30\n",
      "551/551 [==============================] - 837s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4468 - val_accuracy: 0.8357\n",
      "Epoch 16/30\n",
      "551/551 [==============================] - 833s 2s/step - loss: 0.4532 - accuracy: 0.8317 - val_loss: 0.4454 - val_accuracy: 0.8365\n",
      "Epoch 17/30\n",
      "551/551 [==============================] - 832s 2s/step - loss: 0.4527 - accuracy: 0.8320 - val_loss: 0.4443 - val_accuracy: 0.8373\n",
      "Epoch 18/30\n",
      "551/551 [==============================] - 830s 2s/step - loss: 0.4526 - accuracy: 0.8321 - val_loss: 0.4454 - val_accuracy: 0.8365\n",
      "Epoch 19/30\n",
      "551/551 [==============================] - 829s 2s/step - loss: 0.4527 - accuracy: 0.8320 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 20/30\n",
      "551/551 [==============================] - 831s 2s/step - loss: 0.4529 - accuracy: 0.8319 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 21/30\n",
      "551/551 [==============================] - 830s 2s/step - loss: 0.4528 - accuracy: 0.8320 - val_loss: 0.4454 - val_accuracy: 0.8365\n",
      "Epoch 22/30\n",
      "551/551 [==============================] - 830s 2s/step - loss: 0.4528 - accuracy: 0.8320 - val_loss: 0.4417 - val_accuracy: 0.8389\n",
      "Epoch 23/30\n",
      "551/551 [==============================] - 830s 2s/step - loss: 0.4527 - accuracy: 0.8320 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 24/30\n",
      "551/551 [==============================] - 831s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4454 - val_accuracy: 0.8365\n",
      "Epoch 25/30\n",
      "551/551 [==============================] - 829s 2s/step - loss: 0.4528 - accuracy: 0.8320 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 26/30\n",
      "551/551 [==============================] - 831s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4442 - val_accuracy: 0.8373\n",
      "Epoch 27/30\n",
      "551/551 [==============================] - 830s 2s/step - loss: 0.4527 - accuracy: 0.8321 - val_loss: 0.4455 - val_accuracy: 0.8365\n",
      "Epoch 28/30\n",
      "551/551 [==============================] - 832s 2s/step - loss: 0.4527 - accuracy: 0.8321 - val_loss: 0.4455 - val_accuracy: 0.8365\n",
      "Epoch 29/30\n",
      "551/551 [==============================] - 831s 2s/step - loss: 0.4527 - accuracy: 0.8320 - val_loss: 0.4480 - val_accuracy: 0.8349\n",
      "Epoch 30/30\n",
      "551/551 [==============================] - 831s 2s/step - loss: 0.4526 - accuracy: 0.8321 - val_loss: 0.4454 - val_accuracy: 0.8365\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Definir o caminho para as pastas de treinamento e validação\n",
    "train_data_dir = 'train'\n",
    "valid_data_dir = 'validation'\n",
    "\n",
    "# Definir o número de classes\n",
    "num_classes = 2\n",
    "\n",
    "# Definir o tamanho da imagem de entrada\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "# Criar uma instância da ResNet50 pré-treinada\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "# Congelar os pesos da ResNet50 para que não sejam atualizados durante o treinamento\n",
    "base_model.trainable = False\n",
    "\n",
    "# Criar o modelo sequencial\n",
    "model = Sequential()\n",
    "\n",
    "# Adicionar a ResNet50 como base\n",
    "model.add(base_model)\n",
    "\n",
    "# Adicionar camadas adicionais para a rede profunda\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Definir os hiperparâmetros do treinamento\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Pré-processamento dos dados de treinamento e validação\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_data_dir,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Treinamento do modelo\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,  \n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "model.save('resnet_modelFinal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo gasto na versão com CPU (2 threads): 3.3800973296165466 minutos\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from concurrent import futures\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "\n",
    "def aumentarDados(imagem):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        iaa.GammaContrast((0.8, 1.2)),\n",
    "        iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25),\n",
    "        iaa.Affine(scale=(0.8, 1.2))\n",
    "    ])\n",
    "\n",
    "    imagens_aumentadas = seq.augment_images([imagem] * 5)  # Gerar 5 imagens aumentadas\n",
    "\n",
    "    return imagens_aumentadas\n",
    "\n",
    "\n",
    "def processar_imagem(arquivo, pasta_origem, pasta_destino):\n",
    "    caminho_origem = os.path.join(pasta_origem, arquivo)\n",
    "    imagem = cv2.imread(caminho_origem)\n",
    "    imagens_aumentadas = aumentarDados(imagem)\n",
    "    for i, img_aumentada in enumerate(imagens_aumentadas):\n",
    "        nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "        caminho_destino = os.path.join(pasta_destino, nome_arquivo)\n",
    "        cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "\n",
    "def aplicar_aumento_dados(pasta_train):\n",
    "    pasta_destino_aumentadas_0 = os.path.join(pasta_train, \"0\")\n",
    "    pasta_destino_aumentadas_1 = os.path.join(pasta_train, \"1\")\n",
    "\n",
    "    # Criar os diretórios de destino se não existirem\n",
    "    if not os.path.exists(pasta_destino_aumentadas_0):\n",
    "        os.makedirs(pasta_destino_aumentadas_0)\n",
    "    if not os.path.exists(pasta_destino_aumentadas_1):\n",
    "        os.makedirs(pasta_destino_aumentadas_1)\n",
    "\n",
    "    # Percorrer a pasta \"0\" e aplicar aumento de dados nas imagens\n",
    "    pasta_origem_0 = os.path.join(pasta_train, \"0\")\n",
    "    imagens_0 = [arquivo for arquivo in os.listdir(pasta_origem_0) if arquivo.endswith(\".png\")]\n",
    "    with futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        for arquivo in imagens_0:\n",
    "            executor.submit(processar_imagem, arquivo, pasta_origem_0, pasta_destino_aumentadas_0)\n",
    "\n",
    "    # Percorrer a pasta \"1\" e aplicar aumento de dados nas imagens\n",
    "    pasta_origem_1 = os.path.join(pasta_train, \"1\")\n",
    "    imagens_1 = [arquivo for arquivo in os.listdir(pasta_origem_1) if arquivo.endswith(\".png\")]\n",
    "    with futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        for arquivo in imagens_1:\n",
    "            executor.submit(processar_imagem, arquivo, pasta_origem_1, pasta_destino_aumentadas_1)\n",
    "\n",
    "\n",
    "pasta_train = \"train2\"\n",
    "start_time = time.time()\n",
    "aplicar_aumento_dados(pasta_train)\n",
    "cpu_time = time.time() - start_time\n",
    "\n",
    "tempo_minutos = cpu_time / 60\n",
    "print(\"Tempo gasto na versão com CPU (2 threads):\", tempo_minutos, \"minutos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:539: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "ename": "TypingError",
     "evalue": "Failed in cuda mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mUnknown attribute 'Sequential' of type Module(<module 'imgaug.augmenters' from 'c:\\\\Users\\\\felipe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\imgaug\\\\augmenters\\\\__init__.py'>)\n\u001b[1m\nFile \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_8636\\304729065.py\", line 10:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of get attribute at C:\\Users\\felipe\\AppData\\Local\\Temp\\ipykernel_8636\\304729065.py (10)\u001b[0m\n\u001b[1m\nFile \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_8636\\304729065.py\", line 10:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m pasta_train \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 105\u001b[0m aplicar_aumento_dados_GPU(pasta_train)\n\u001b[0;32m    106\u001b[0m gpu_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    108\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTempo gasto na versão com GPU:\u001b[39m\u001b[39m\"\u001b[39m, gpu_time)\n",
      "Cell \u001b[1;32mIn[42], line 60\u001b[0m, in \u001b[0;36maplicar_aumento_dados_GPU\u001b[1;34m(pasta_train)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m# Executar função em GPU\u001b[39;00m\n\u001b[0;32m     59\u001b[0m blocks_per_grid \u001b[39m=\u001b[39m (imagens_0\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m5\u001b[39m \u001b[39m+\u001b[39m threads_per_block \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m threads_per_block\n\u001b[1;32m---> 60\u001b[0m aumentarDados_GPU[blocks_per_grid, threads_per_block](d_imagens_0, d_imagens_aumentadas_0)\n\u001b[0;32m     62\u001b[0m \u001b[39m# Copiar resultado de volta para a CPU\u001b[39;00m\n\u001b[0;32m     63\u001b[0m d_imagens_aumentadas_0\u001b[39m.\u001b[39mcopy_to_host(imagens_aumentadas_0)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:542\u001b[0m, in \u001b[0;36m_LaunchConfiguration.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatcher\u001b[39m.\u001b[39;49mcall(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgriddim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblockdim,\n\u001b[0;32m    543\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msharedmem)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:676\u001b[0m, in \u001b[0;36mCUDADispatcher.call\u001b[1;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[0;32m    674\u001b[0m     kernel \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverloads\u001b[39m.\u001b[39mvalues()))\n\u001b[0;32m    675\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 676\u001b[0m     kernel \u001b[39m=\u001b[39m _dispatcher\u001b[39m.\u001b[39;49mDispatcher\u001b[39m.\u001b[39;49m_cuda_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    678\u001b[0m kernel\u001b[39m.\u001b[39mlaunch(args, griddim, blockdim, stream, sharedmem)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:684\u001b[0m, in \u001b[0;36mCUDADispatcher._compile_for_args\u001b[1;34m(self, *args, **kws)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m kws\n\u001b[0;32m    683\u001b[0m argtypes \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtypeof_pyval(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[1;32m--> 684\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile(\u001b[39mtuple\u001b[39;49m(argtypes))\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:927\u001b[0m, in \u001b[0;36mCUDADispatcher.compile\u001b[1;34m(self, sig)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_compile:\n\u001b[0;32m    925\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCompilation disabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 927\u001b[0m kernel \u001b[39m=\u001b[39m _Kernel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpy_func, argtypes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargetoptions)\n\u001b[0;32m    928\u001b[0m \u001b[39m# We call bind to force codegen, so that there is a cubin to cache\u001b[39;00m\n\u001b[0;32m    929\u001b[0m kernel\u001b[39m.\u001b[39mbind()\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_acquire_compile_lock\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:84\u001b[0m, in \u001b[0;36m_Kernel.__init__\u001b[1;34m(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)\u001b[0m\n\u001b[0;32m     78\u001b[0m nvvm_options \u001b[39m=\u001b[39m {\n\u001b[0;32m     79\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfastmath\u001b[39m\u001b[39m'\u001b[39m: fastmath,\n\u001b[0;32m     80\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mopt\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m \u001b[39mif\u001b[39;00m opt \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m     81\u001b[0m }\n\u001b[0;32m     83\u001b[0m cc \u001b[39m=\u001b[39m get_current_device()\u001b[39m.\u001b[39mcompute_capability\n\u001b[1;32m---> 84\u001b[0m cres \u001b[39m=\u001b[39m compile_cuda(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpy_func, types\u001b[39m.\u001b[39;49mvoid, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margtypes,\n\u001b[0;32m     85\u001b[0m                     debug\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdebug,\n\u001b[0;32m     86\u001b[0m                     lineinfo\u001b[39m=\u001b[39;49mlineinfo,\n\u001b[0;32m     87\u001b[0m                     inline\u001b[39m=\u001b[39;49minline,\n\u001b[0;32m     88\u001b[0m                     fastmath\u001b[39m=\u001b[39;49mfastmath,\n\u001b[0;32m     89\u001b[0m                     nvvm_options\u001b[39m=\u001b[39;49mnvvm_options,\n\u001b[0;32m     90\u001b[0m                     cc\u001b[39m=\u001b[39;49mcc)\n\u001b[0;32m     91\u001b[0m tgt_ctx \u001b[39m=\u001b[39m cres\u001b[39m.\u001b[39mtarget_context\n\u001b[0;32m     92\u001b[0m code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpy_func\u001b[39m.\u001b[39m\u001b[39m__code__\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_acquire_compile_lock\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\compiler.py:230\u001b[0m, in \u001b[0;36mcompile_cuda\u001b[1;34m(pyfunc, return_type, args, debug, lineinfo, inline, fastmath, nvvm_options, cc)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtarget_extension\u001b[39;00m \u001b[39mimport\u001b[39;00m target_override\n\u001b[0;32m    229\u001b[0m \u001b[39mwith\u001b[39;00m target_override(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 230\u001b[0m     cres \u001b[39m=\u001b[39m compiler\u001b[39m.\u001b[39;49mcompile_extra(typingctx\u001b[39m=\u001b[39;49mtypingctx,\n\u001b[0;32m    231\u001b[0m                                   targetctx\u001b[39m=\u001b[39;49mtargetctx,\n\u001b[0;32m    232\u001b[0m                                   func\u001b[39m=\u001b[39;49mpyfunc,\n\u001b[0;32m    233\u001b[0m                                   args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    234\u001b[0m                                   return_type\u001b[39m=\u001b[39;49mreturn_type,\n\u001b[0;32m    235\u001b[0m                                   flags\u001b[39m=\u001b[39;49mflags,\n\u001b[0;32m    236\u001b[0m                                   \u001b[39mlocals\u001b[39;49m\u001b[39m=\u001b[39;49m{},\n\u001b[0;32m    237\u001b[0m                                   pipeline_class\u001b[39m=\u001b[39;49mCUDACompiler)\n\u001b[0;32m    239\u001b[0m library \u001b[39m=\u001b[39m cres\u001b[39m.\u001b[39mlibrary\n\u001b[0;32m    240\u001b[0m library\u001b[39m.\u001b[39mfinalize()\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler.py:742\u001b[0m, in \u001b[0;36mcompile_extra\u001b[1;34m(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compiler entry point\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \n\u001b[0;32m    720\u001b[0m \u001b[39mParameter\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[39m    compiler pipeline\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    740\u001b[0m pipeline \u001b[39m=\u001b[39m pipeline_class(typingctx, targetctx, library,\n\u001b[0;32m    741\u001b[0m                           args, return_type, flags, \u001b[39mlocals\u001b[39m)\n\u001b[1;32m--> 742\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline\u001b[39m.\u001b[39;49mcompile_extra(func)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler.py:460\u001b[0m, in \u001b[0;36mCompilerBase.compile_extra\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mlifted \u001b[39m=\u001b[39m ()\n\u001b[0;32m    459\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mlifted_from \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compile_bytecode()\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler.py:528\u001b[0m, in \u001b[0;36mCompilerBase._compile_bytecode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39mPopulate and run pipeline for bytecode input\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfunc_ir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 528\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compile_core()\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler.py:507\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus\u001b[39m.\u001b[39mfail_reason \u001b[39m=\u001b[39m e\n\u001b[0;32m    506\u001b[0m         \u001b[39mif\u001b[39;00m is_final_pipeline:\n\u001b[1;32m--> 507\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    508\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m CompilerError(\u001b[39m\"\u001b[39m\u001b[39mAll available pipelines exhausted\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler.py:494\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 494\u001b[0m     pm\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mcr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_machinery.py:368\u001b[0m, in \u001b[0;36mPassManager.run\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    365\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFailed in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m mode pipeline (step: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \\\n\u001b[0;32m    366\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline_name, pass_desc)\n\u001b[0;32m    367\u001b[0m patched_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_patch_error(msg, e)\n\u001b[1;32m--> 368\u001b[0m \u001b[39mraise\u001b[39;00m patched_exception\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_machinery.py:356\u001b[0m, in \u001b[0;36mPassManager.run\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    354\u001b[0m pass_inst \u001b[39m=\u001b[39m _pass_registry\u001b[39m.\u001b[39mget(pss)\u001b[39m.\u001b[39mpass_inst\n\u001b[0;32m    355\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(pass_inst, CompilerPass):\n\u001b[1;32m--> 356\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runPass(idx, pass_inst, state)\n\u001b[0;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mBaseException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLegacy pass in use\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_acquire_compile_lock\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_machinery.py:311\u001b[0m, in \u001b[0;36mPassManager._runPass\u001b[1;34m(self, index, pss, internal_state)\u001b[0m\n\u001b[0;32m    309\u001b[0m     mutated \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m check(pss\u001b[39m.\u001b[39mrun_initialization, internal_state)\n\u001b[0;32m    310\u001b[0m \u001b[39mwith\u001b[39;00m SimpleTimer() \u001b[39mas\u001b[39;00m pass_time:\n\u001b[1;32m--> 311\u001b[0m     mutated \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m check(pss\u001b[39m.\u001b[39;49mrun_pass, internal_state)\n\u001b[0;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m SimpleTimer() \u001b[39mas\u001b[39;00m finalize_time:\n\u001b[0;32m    313\u001b[0m     mutated \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m check(pss\u001b[39m.\u001b[39mrun_finalizer, internal_state)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_machinery.py:273\u001b[0m, in \u001b[0;36mPassManager._runPass.<locals>.check\u001b[1;34m(func, compiler_state)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck\u001b[39m(func, compiler_state):\n\u001b[1;32m--> 273\u001b[0m     mangled \u001b[39m=\u001b[39m func(compiler_state)\n\u001b[0;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m mangled \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    275\u001b[0m         msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mCompilerPass implementations should return True/False. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m                \u001b[39m\"\u001b[39m\u001b[39mCompilerPass with name \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m did not.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\typed_passes.py:110\u001b[0m, in \u001b[0;36mBaseTypeInference.run_pass\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39mType inference and legalization\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mwith\u001b[39;00m fallback_context(state, \u001b[39m'\u001b[39m\u001b[39mFunction \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m failed type inference\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    108\u001b[0m                       \u001b[39m%\u001b[39m (state\u001b[39m.\u001b[39mfunc_id\u001b[39m.\u001b[39mfunc_name,)):\n\u001b[0;32m    109\u001b[0m     \u001b[39m# Type inference\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     typemap, return_type, calltypes, errs \u001b[39m=\u001b[39m type_inference_stage(\n\u001b[0;32m    111\u001b[0m         state\u001b[39m.\u001b[39;49mtypingctx,\n\u001b[0;32m    112\u001b[0m         state\u001b[39m.\u001b[39;49mtargetctx,\n\u001b[0;32m    113\u001b[0m         state\u001b[39m.\u001b[39;49mfunc_ir,\n\u001b[0;32m    114\u001b[0m         state\u001b[39m.\u001b[39;49margs,\n\u001b[0;32m    115\u001b[0m         state\u001b[39m.\u001b[39;49mreturn_type,\n\u001b[0;32m    116\u001b[0m         state\u001b[39m.\u001b[39;49mlocals,\n\u001b[0;32m    117\u001b[0m         raise_errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_errors)\n\u001b[0;32m    118\u001b[0m     state\u001b[39m.\u001b[39mtypemap \u001b[39m=\u001b[39m typemap\n\u001b[0;32m    119\u001b[0m     \u001b[39m# save errors in case of partial typing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\typed_passes.py:88\u001b[0m, in \u001b[0;36mtype_inference_stage\u001b[1;34m(typingctx, targetctx, interp, args, return_type, locals, raise_errors)\u001b[0m\n\u001b[0;32m     86\u001b[0m     infer\u001b[39m.\u001b[39mbuild_constraint()\n\u001b[0;32m     87\u001b[0m     \u001b[39m# return errors in case of partial typing\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     errs \u001b[39m=\u001b[39m infer\u001b[39m.\u001b[39;49mpropagate(raise_errors\u001b[39m=\u001b[39;49mraise_errors)\n\u001b[0;32m     89\u001b[0m     typemap, restype, calltypes \u001b[39m=\u001b[39m infer\u001b[39m.\u001b[39munify(raise_errors\u001b[39m=\u001b[39mraise_errors)\n\u001b[0;32m     91\u001b[0m \u001b[39m# Output all Numba warnings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\typeinfer.py:1086\u001b[0m, in \u001b[0;36mTypeInferer.propagate\u001b[1;34m(self, raise_errors)\u001b[0m\n\u001b[0;32m   1083\u001b[0m force_lit_args \u001b[39m=\u001b[39m [e \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m errors\n\u001b[0;32m   1084\u001b[0m                   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, ForceLiteralArg)]\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m force_lit_args:\n\u001b[1;32m-> 1086\u001b[0m     \u001b[39mraise\u001b[39;00m errors[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1087\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m reduce(operator\u001b[39m.\u001b[39mor_, force_lit_args)\n",
      "\u001b[1;31mTypingError\u001b[0m: Failed in cuda mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mUnknown attribute 'Sequential' of type Module(<module 'imgaug.augmenters' from 'c:\\\\Users\\\\felipe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\imgaug\\\\augmenters\\\\__init__.py'>)\n\u001b[1m\nFile \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_8636\\304729065.py\", line 10:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of get attribute at C:\\Users\\felipe\\AppData\\Local\\Temp\\ipykernel_8636\\304729065.py (10)\u001b[0m\n\u001b[1m\nFile \"..\\..\\..\\..\\..\\AppData\\Local\\Temp\\ipykernel_8636\\304729065.py\", line 10:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from numba import cuda\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# Função para aumentar os dados em GPU\n",
    "@cuda.jit\n",
    "def aumentarDados_GPU(imagens, imagens_aumentadas):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        iaa.GammaContrast((0.8, 1.2)),\n",
    "        iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25),\n",
    "        iaa.Affine(scale=(0.8, 1.2))\n",
    "    ])\n",
    "\n",
    "    # Índice global do thread\n",
    "    i = cuda.grid(1)\n",
    "\n",
    "    # Realizar aumento de dados em lote\n",
    "    for k in range(5):\n",
    "        if i < imagens.shape[0]:\n",
    "            imagem = imagens[i]\n",
    "            img_aumentada = seq.augment_image(imagem)\n",
    "            imagens_aumentadas[i + k * imagens.shape[0]] = img_aumentada\n",
    "\n",
    "# Função para aplicar aumento de dados em GPU\n",
    "def aplicar_aumento_dados_GPU(pasta_train):\n",
    "    pasta_destino_aumentadas_0 = os.path.join(pasta_train, \"0\")\n",
    "    pasta_destino_aumentadas_1 = os.path.join(pasta_train, \"1\")\n",
    "\n",
    "    # Criar os diretórios de destino se não existirem\n",
    "    if not os.path.exists(pasta_destino_aumentadas_0):\n",
    "        os.makedirs(pasta_destino_aumentadas_0)\n",
    "    if not os.path.exists(pasta_destino_aumentadas_1):\n",
    "        os.makedirs(pasta_destino_aumentadas_1)\n",
    "\n",
    "    # Configuração do dispositivo CUDA\n",
    "    threads_per_block = cuda.get_current_device().MAX_THREADS_PER_BLOCK\n",
    "\n",
    "    # Percorrer a pasta \"0\" e aplicar aumento de dados nas imagens\n",
    "    pasta_origem_0 = os.path.join(pasta_train, \"0\")\n",
    "    imagens_0 = []\n",
    "    for arquivo in os.listdir(pasta_origem_0):\n",
    "        if arquivo.endswith(\".png\"):\n",
    "            caminho_origem = os.path.join(pasta_origem_0, arquivo)\n",
    "            imagem = cv2.imread(caminho_origem, cv2.IMREAD_COLOR)\n",
    "            imagens_0.append(imagem)\n",
    "\n",
    "    imagens_0 = np.array(imagens_0)\n",
    "    imagens_aumentadas_0 = np.empty((imagens_0.shape[0] * 5, *imagens_0.shape[1:]), dtype=np.uint8)\n",
    "\n",
    "    # Alocação de memória na GPU\n",
    "    d_imagens_0 = cuda.to_device(imagens_0)\n",
    "    d_imagens_aumentadas_0 = cuda.to_device(imagens_aumentadas_0)\n",
    "\n",
    "    # Executar função em GPU\n",
    "    blocks_per_grid = (imagens_0.shape[0] * 5 + threads_per_block - 1) // threads_per_block\n",
    "    aumentarDados_GPU[blocks_per_grid, threads_per_block](d_imagens_0, d_imagens_aumentadas_0)\n",
    "\n",
    "    # Copiar resultado de volta para a CPU\n",
    "    d_imagens_aumentadas_0.copy_to_host(imagens_aumentadas_0)\n",
    "\n",
    "    # Salvar imagens aumentadas\n",
    "    for i, img_aumentada in enumerate(imagens_aumentadas_0):\n",
    "        arquivo = os.listdir(pasta_origem_0)[i % imagens_0.shape[0]]\n",
    "        nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "        caminho_destino = os.path.join(pasta_destino_aumentadas_0, nome_arquivo)\n",
    "        cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "    # Percorrer a pasta \"1\" e aplicar aumento de dados nas imagens\n",
    "    pasta_origem_1 = os.path.join(pasta_train, \"1\")\n",
    "    imagens_1 = []\n",
    "    for arquivo in os.listdir(pasta_origem_1):\n",
    "        if arquivo.endswith(\".png\"):\n",
    "            caminho_origem = os.path.join(pasta_origem_1, arquivo)\n",
    "            imagem = cv2.imread(caminho_origem, cv2.IMREAD_COLOR)\n",
    "            imagens_1.append(imagem)\n",
    "\n",
    "    imagens_1 = np.array(imagens_1)\n",
    "    imagens_aumentadas_1 = np.empty((imagens_1.shape[0] * 5, *imagens_1.shape[1:]), dtype=np.uint8)\n",
    "\n",
    "    # Alocação de memória na GPU\n",
    "    d_imagens_1 = cuda.to_device(imagens_1)\n",
    "    d_imagens_aumentadas_1 = cuda.to_device(imagens_aumentadas_1)\n",
    "\n",
    "    # Executar função em GPU\n",
    "    blocks_per_grid = (imagens_1.shape[0] * 5 + threads_per_block - 1) // threads_per_block\n",
    "    aumentarDados_GPU[blocks_per_grid, threads_per_block](d_imagens_1, d_imagens_aumentadas_1)\n",
    "\n",
    "    # Copiar resultado de volta para a CPU\n",
    "    d_imagens_aumentadas_1.copy_to_host(imagens_aumentadas_1)\n",
    "\n",
    "    # Salvar imagens aumentadas\n",
    "    for i, img_aumentada in enumerate(imagens_aumentadas_1):\n",
    "        arquivo = os.listdir(pasta_origem_1)[i % imagens_1.shape[0]]\n",
    "        nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "        caminho_destino = os.path.join(pasta_destino_aumentadas_1, nome_arquivo)\n",
    "        cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "\n",
    "pasta_train = \"train2\"\n",
    "start_time = time.time()\n",
    "aplicar_aumento_dados_GPU(pasta_train)\n",
    "gpu_time = time.time() - start_time\n",
    "\n",
    "print(\"Tempo gasto na versão com GPU:\", gpu_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
