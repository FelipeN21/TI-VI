{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "def aumentarDados(imagem):\n",
    "    seq = iaa.Sequential([\n",
    "        #iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        #iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        #iaa.Multiply((0.8, 1.2)),\n",
    "        iaa.GammaContrast((0.2, 0.9)),\n",
    "        #iaa.Affine(scale=(0.8, 1.2))\n",
    "    ])\n",
    "    return seq.augment_image(imagem)\n",
    "\n",
    "img = cv2.imread(\"imagem.png\")\n",
    "img_cinza = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "imagens_aumentadas = []\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    img_aumentada = aumentarDados(img_cinza)\n",
    "    imagens_aumentadas.append(img_aumentada)\n",
    "    nome_arquivo = \"teste/imagem_aumentada_\" + str(i+1) + \".png\"\n",
    "    cv2.imwrite(nome_arquivo, img_aumentada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "p1 = 'TB_Chest_Radiography_Database/Tuberculosis/Tuberculosis-21.png'\n",
    "\n",
    "def equalize_histogram(img):\n",
    "    img_eq = cv2.equalizeHist(img)\n",
    "    return img_eq\n",
    "\n",
    "\n",
    "img = cv2.imread(p1, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "img_eq = equalize_histogram(img)\n",
    "clahe=cv2.createCLAHE(clipLimit=20)\n",
    "eg=clahe.apply(img_eq)\n",
    "eqhist_images=np.concatenate((img,eg),axis=1)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(eqhist_images,cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ic = cv2.bitwise_not(img)\n",
    "eqhist_images=np.concatenate((img,ic),axis=1)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(eqhist_images,cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_path = 'TB_Chest_Radiography_Database/Tuberculosis/Tuberculosis-1.png'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "gray_img = gray_img/255.0\n",
    "im_power_law_transformation = cv2.pow(gray_img, 0.6)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.hstack((gray_img*255, im_power_law_transformation*255)),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_eq = cv2.equalizeHist(img)\n",
    "eg= cv2.GaussianBlur(eg, (5,5), 0)\n",
    "_, thresh = cv2.threshold(eg, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "dist_transform = cv2.distanceTransform(thresh, cv2.DIST_L2, 5)\n",
    "_, sure_fg = cv2.threshold(dist_transform, 0.5*dist_transform.max(), 255, 0)\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(thresh, sure_fg)\n",
    "\n",
    "# Threshold eg image to obtain a binary image\n",
    "_, binary_eg = cv2.threshold(eg, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "_, markers = cv2.connectedComponents(binary_eg)\n",
    "markers = markers + 1\n",
    "markers[unknown==255] = 0\n",
    "\n",
    "# Convert eg to 3-channel image\n",
    "eg = cv2.cvtColor(eg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "markers = cv2.watershed(eg, markers)\n",
    "eg[markers == -1] = 255\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(eg,cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = 'TB_Chest_Radiography_Database/Tuberculosis'\n",
    "output_dir = 'turberculoseC'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "image_files = os.listdir(image_dir)\n",
    "\n",
    "for imag_file in image_files:\n",
    "    imag_P = os.path.join(image_dir,imag_file)\n",
    "    \n",
    "\n",
    "    if img is None:\n",
    "        print(\"Erro\")\n",
    "        continue\n",
    "\n",
    "    #img_Red = cv2.resize(img, (0, 0), fx=0.6, fy=0.6)\n",
    "    #img_g = cv2.GaussianBlur(img_Red, (5, 5), 0)\n",
    "    #gray = cv2.cvtColor(img_g, cv2.COLOR_BGR2GRAY)\n",
    "    #thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    #kernel = np.ones((3, 3), np.uint8)\n",
    "    #opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "    #contours, hierarchy = cv2.findContours(opening, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #cv2.drawContours(img, contours, -1, (0, 255, 0), 2)\n",
    "    img = cv2.imread(imag_P, cv2.IMREAD_GRAYSCALE)\n",
    "    eg = cv2.equalizeHist(img)\n",
    "    clahe=cv2.createCLAHE(clipLimit=5)\n",
    "    eg=clahe.apply(eg)\n",
    "    eg= cv2.GaussianBlur(eg, (5,5), 0)\n",
    "    _, thresh = cv2.threshold(eg, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    dist_transform = cv2.distanceTransform(thresh, cv2.DIST_L2, 5)\n",
    "    _, sure_fg = cv2.threshold(dist_transform, 0.5*dist_transform.max(), 255, 0)\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(thresh, sure_fg)\n",
    "\n",
    "    # Threshold eg image to obtain a binary image\n",
    "    _, binary_eg = cv2.threshold(eg, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "    _, markers = cv2.connectedComponents(binary_eg)\n",
    "    markers = markers + 1\n",
    "    markers[unknown==255] = 0\n",
    "\n",
    "    # Convert eg to 3-channel image\n",
    "    eg = cv2.cvtColor(eg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    markers = cv2.watershed(eg, markers)\n",
    "    eg[markers == -1] = 255\n",
    "\n",
    "    output_file = os.path.join(output_dir, f'resized_{imag_file}')\n",
    "    cv2.imwrite(output_file, eg)\n",
    "\n",
    "\n",
    "    # img_Red=cv2.resize(img,(0,0), fx=0.6, fy=0.6)\n",
    "    # #plt.imshow(img_Red)\n",
    "    # img_g= cv2.GaussianBlur(img_Red,(5,5),0)\n",
    "    # #plt.imshow(img_g)\n",
    "    # gray = cv2.cvtColor(img_g, cv2.COLOR_BGR2GRAY)\n",
    "    # ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    # kernel = np.ones((3,3),np.uint8)\n",
    "    # erosion = cv2.erode(thresh,kernel,iterations = 1)\n",
    "    # dilation = cv2.dilate(erosion,kernel,iterations = 1)\n",
    "    # contours, hierarchy = cv2.findContours(dilation, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # cv2.drawContours(img, contours, -1, (0,255,0), 2)\n",
    "    # output_file = os.path.join(output_dir, f'resized_{imag_file}')\n",
    "    # cv2.imwrite(output_file,dilation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Definindo diretórios\n",
    "p1 = 'turberculoseC'\n",
    "p2 = 'turberculoseN'\n",
    "train_dir = 'train2'\n",
    "validation_dir = 'validation2'\n",
    "\n",
    "# Criando as pastas train e validation\n",
    "if not os.path.exists(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "\n",
    "if not os.path.exists(validation_dir):\n",
    "    os.makedirs(validation_dir)\n",
    "\n",
    "# Definindo o tamanho do conjunto de treino e validação\n",
    "train_size = 0.7\n",
    "validation_size = 0.3\n",
    "\n",
    "# Mesclando as imagens das pastas p1 e p2 de forma aleatória\n",
    "images = []\n",
    "for img_name in os.listdir(p1):\n",
    "    images.append((os.path.join(p1, img_name), 1))\n",
    "for img_name in os.listdir(p2):\n",
    "    images.append((os.path.join(p2, img_name), 0))\n",
    "random.shuffle(images)\n",
    "\n",
    "# Copiando as imagens mescladas para as pastas train e validation\n",
    "for i, (image_path, label) in enumerate(images):\n",
    "    if i < len(images) * train_size:\n",
    "        dst_dir = os.path.join(train_dir, str(label))\n",
    "    else:\n",
    "        dst_dir = os.path.join(validation_dir, str(label))\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    shutil.copy(image_path, os.path.join(dst_dir, os.path.basename(image_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo diretórios\n",
    "train_dir = 'train'\n",
    "validation_dir = 'validation'\n",
    "\n",
    "# Definindo número de classes e tamanho das imagens\n",
    "num_classes = 2\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Definindo o tamanho do batch de imagens a ser utilizado no treinamento\n",
    "batch_size = 30\n",
    "\n",
    "# Definindo geradores de imagens para treinamento e validação com aumento de dados\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.4,\n",
    "                                   zoom_range=0.4,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Criando geradores de imagens a partir dos diretórios\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=(img_width, img_height),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(validation_dir,\n",
    "                                                       target_size=(img_width, img_height),\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       class_mode='categorical')\n",
    "\n",
    "# Definindo a arquitetura da ResNet50 com pesos pré-treinados no ImageNet\n",
    "resnet_model = ResNet50(include_top=False, weights='imagenet',\n",
    "                        input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Congelando as camadas convolucionais da ResNet50\n",
    "for layer in resnet_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Adicionando camadas densas para classificação\n",
    "x = resnet_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Definindo o modelo final\n",
    "model = tf.keras.models.Model(inputs=resnet_model.input, outputs=output)\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Treinando o modelo\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=train_generator.n // batch_size,\n",
    "                    epochs=25,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_generator.n // batch_size)\n",
    "\n",
    "# Avaliando o modelo na validação\n",
    "scores = model.evaluate(validation_generator)\n",
    "print(f'Acurácia na validação: {scores[1]*100}%')\n",
    "\n",
    "# Salvando o modelo treinado\n",
    "model.save('tb_detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"TB_Chest_Radiography_Database/Tuberculosis/Tuberculosis-1.png\")\n",
    "\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)[1]\n",
    "\n",
    "\n",
    "model = cv2.dnn.readNetFromTensorflow(\"deeplab_v3_plus.pb\")\n",
    "blob = cv2.dnn.blobFromImage(img, swapRB=True, crop=False)\n",
    "model.setInput(blob)\n",
    "output = model.forward()\n",
    "output = output[0, :, :, :]\n",
    "classes = cv2.imread(\"labels.png\")\n",
    "classes = cv2.resize(classes, (output.shape[1], output.shape[0]))\n",
    "classes = cv2.cvtColor(classes, cv2.COLOR_BGR2GRAY)\n",
    "classes = np.array(classes)\n",
    "classes = classes[:, :, np.newaxis]\n",
    "colors = np.array([[0, 0, 0], [128, 0, 0], [0, 128, 0]])\n",
    "mask = np.argmax(output, axis=2)\n",
    "mask = colors[mask]\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_RGB2BGR)\n",
    "mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "mask = cv2.addWeighted(img, 0.5, mask, 0.5, 0)\n",
    "cv2.imshow(\"dd\", mask)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m proporcao_treinamento \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[39m# Dividir as bases de dados e gerar as imagens aumentadas\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m dividir_bases_dados(pasta_tuberculose, pasta_sem_tuberculose, proporcao_treinamento)\n",
      "Cell \u001b[1;32mIn[70], line 67\u001b[0m, in \u001b[0;36mdividir_bases_dados\u001b[1;34m(pasta_tuberculose, pasta_sem_tuberculose, proporcao_treinamento)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m# Gerar imagens aumentadas para o conjunto de treinamento com tuberculose\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m img_tuberculose \u001b[39min\u001b[39;00m imagens_tuberculose_treinamento:\n\u001b[1;32m---> 67\u001b[0m     aumentarDados(img_tuberculose, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     69\u001b[0m \u001b[39m# Gerar imagens aumentadas para o conjunto de treinamento sem tuberculose\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m img_sem_tuberculose \u001b[39min\u001b[39;00m imagens_sem_tuberculose_treinamento:\n",
      "Cell \u001b[1;32mIn[70], line 31\u001b[0m, in \u001b[0;36maumentarDados\u001b[1;34m(imagem, tem_tuberculose)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m# Salvar a imagem\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mif\u001b[39;00m tem_tuberculose:\n\u001b[1;32m---> 31\u001b[0m     cv2\u001b[39m.\u001b[39;49mimwrite(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain/imagem_aumentada_tuberculose_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m\"\u001b[39;49m, img_eq)\n\u001b[0;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     cv2\u001b[39m.\u001b[39mimwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain/imagem_aumentada_sem_tuberculose_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m, )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from imgaug import augmenters as iaa\n",
    "import random\n",
    "\n",
    "def aumentarDados(imagem, tem_tuberculose):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        iaa.GammaContrast((0.8, 1.2)),\n",
    "        iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25),\n",
    "        iaa.Affine(scale=(0.8, 1.2))\n",
    "    ])\n",
    "\n",
    " \n",
    "    imagens_aumentadas = seq.augment_images([imagem] * 5)  # Gerar 10 imagens aumentadas\n",
    "\n",
    "    # Salvar as imagens resultantes\n",
    "    for i, img in enumerate(imagens_aumentadas):\n",
    "        # Aplicar Equalização e Limiarização\n",
    "        img_eq = cv2.equalizeHist(img)\n",
    "     \n",
    "     \n",
    "        # Salvar a imagem\n",
    "        if tem_tuberculose:\n",
    "            cv2.imwrite(f\"train/imagem_aumentada_tuberculose_{i}.png\", img_eq)\n",
    "        else:\n",
    "            cv2.imwrite(f\"train/imagem_aumentada_sem_tuberculose_{i}.png\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo gasto na versão sequencial: 352.0161859989166\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "def aumentarDados(imagem):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        iaa.GammaContrast((0.8, 1.2)),\n",
    "        iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25),\n",
    "        iaa.Affine(scale=(0.8, 1.2))\n",
    "    ])\n",
    "\n",
    "    imagens_aumentadas = seq.augment_images([imagem] * 5)  # Gerar 5 imagens aumentadas\n",
    "\n",
    "    return imagens_aumentadas\n",
    "\n",
    "def aplicar_aumento_dados(pasta_train):\n",
    "    pasta_destino_aumentadas_0 = os.path.join(pasta_train, \"0\")\n",
    "    pasta_destino_aumentadas_1 = os.path.join(pasta_train, \"1\")\n",
    "\n",
    "    # Criar os diretórios de destino se não existirem\n",
    "    if not os.path.exists(pasta_destino_aumentadas_0):\n",
    "        os.makedirs(pasta_destino_aumentadas_0)\n",
    "    if not os.path.exists(pasta_destino_aumentadas_1):\n",
    "        os.makedirs(pasta_destino_aumentadas_1)\n",
    "\n",
    "    # Percorrer a pasta \"0\" e aplicar aumento de dados nas imagens\n",
    "    pasta_origem_0 = os.path.join(pasta_train, \"0\")\n",
    "    for arquivo in os.listdir(pasta_origem_0):\n",
    "        if arquivo.endswith(\".png\"):\n",
    "            caminho_origem = os.path.join(pasta_origem_0, arquivo)\n",
    "            imagem = cv2.imread(caminho_origem)\n",
    "            imagens_aumentadas = aumentarDados(imagem)\n",
    "            for i, img_aumentada in enumerate(imagens_aumentadas):\n",
    "                nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "                caminho_destino = os.path.join(pasta_destino_aumentadas_0, nome_arquivo)\n",
    "                cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "    # Percorrer a pasta \"1\" e aplicar aumento de dados nas imagens\n",
    "    pasta_origem_1 = os.path.join(pasta_train, \"1\")\n",
    "    for arquivo in os.listdir(pasta_origem_1):\n",
    "        if arquivo.endswith(\".png\"):\n",
    "            caminho_origem = os.path.join(pasta_origem_1, arquivo)\n",
    "            imagem = cv2.imread(caminho_origem)\n",
    "            imagens_aumentadas = aumentarDados(imagem)\n",
    "            for i, img_aumentada in enumerate(imagens_aumentadas):\n",
    "                nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "                caminho_destino = os.path.join(pasta_destino_aumentadas_1, nome_arquivo)\n",
    "                cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "\n",
    "\n",
    "pasta_train = \"train2\"\n",
    "start_time = time.time()\n",
    "aplicar_aumento_dados(pasta_train)\n",
    "sequencial_time = time.time() - start_time\n",
    "\n",
    "print(\"Tempo gasto na versão sequencial:\", sequencial_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17640 images belonging to 2 classes.\n",
      "Found 1260 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "551/551 [==============================] - 875s 2s/step - loss: 0.5394 - accuracy: 0.8403 - val_loss: 1.0881 - val_accuracy: 0.8349\n",
      "Epoch 2/30\n",
      "551/551 [==============================] - 865s 2s/step - loss: 0.5547 - accuracy: 0.8001 - val_loss: 0.4700 - val_accuracy: 0.8381\n",
      "Epoch 3/30\n",
      "551/551 [==============================] - 903s 2s/step - loss: 0.4623 - accuracy: 0.8318 - val_loss: 0.4498 - val_accuracy: 0.8357\n",
      "Epoch 4/30\n",
      "551/551 [==============================] - 913s 2s/step - loss: 0.4537 - accuracy: 0.8320 - val_loss: 0.4446 - val_accuracy: 0.8373\n",
      "Epoch 5/30\n",
      "551/551 [==============================] - 854s 2s/step - loss: 0.4529 - accuracy: 0.8320 - val_loss: 0.4469 - val_accuracy: 0.8357\n",
      "Epoch 6/30\n",
      "551/551 [==============================] - 855s 2s/step - loss: 0.4525 - accuracy: 0.8322 - val_loss: 0.4455 - val_accuracy: 0.8365\n",
      "Epoch 7/30\n",
      "551/551 [==============================] - 858s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4441 - val_accuracy: 0.8373\n",
      "Epoch 8/30\n",
      "551/551 [==============================] - 875s 2s/step - loss: 0.4525 - accuracy: 0.8321 - val_loss: 0.4480 - val_accuracy: 0.8349\n",
      "Epoch 9/30\n",
      "551/551 [==============================] - 1429s 3s/step - loss: 0.4528 - accuracy: 0.8320 - val_loss: 0.4454 - val_accuracy: 0.8365\n",
      "Epoch 10/30\n",
      "551/551 [==============================] - 835s 2s/step - loss: 0.4528 - accuracy: 0.8320 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 11/30\n",
      "551/551 [==============================] - 836s 2s/step - loss: 0.4526 - accuracy: 0.8321 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 12/30\n",
      "551/551 [==============================] - 834s 2s/step - loss: 0.4531 - accuracy: 0.8318 - val_loss: 0.4442 - val_accuracy: 0.8373\n",
      "Epoch 13/30\n",
      "551/551 [==============================] - 834s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4429 - val_accuracy: 0.8381\n",
      "Epoch 14/30\n",
      "551/551 [==============================] - 834s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4468 - val_accuracy: 0.8357\n",
      "Epoch 15/30\n",
      "551/551 [==============================] - 837s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4468 - val_accuracy: 0.8357\n",
      "Epoch 16/30\n",
      "551/551 [==============================] - 833s 2s/step - loss: 0.4532 - accuracy: 0.8317 - val_loss: 0.4454 - val_accuracy: 0.8365\n",
      "Epoch 17/30\n",
      "551/551 [==============================] - 832s 2s/step - loss: 0.4527 - accuracy: 0.8320 - val_loss: 0.4443 - val_accuracy: 0.8373\n",
      "Epoch 18/30\n",
      "551/551 [==============================] - 830s 2s/step - loss: 0.4526 - accuracy: 0.8321 - val_loss: 0.4454 - val_accuracy: 0.8365\n",
      "Epoch 19/30\n",
      "551/551 [==============================] - 829s 2s/step - loss: 0.4527 - accuracy: 0.8320 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 20/30\n",
      "551/551 [==============================] - 831s 2s/step - loss: 0.4529 - accuracy: 0.8319 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 21/30\n",
      "551/551 [==============================] - 830s 2s/step - loss: 0.4528 - accuracy: 0.8320 - val_loss: 0.4454 - val_accuracy: 0.8365\n",
      "Epoch 22/30\n",
      "551/551 [==============================] - 830s 2s/step - loss: 0.4528 - accuracy: 0.8320 - val_loss: 0.4417 - val_accuracy: 0.8389\n",
      "Epoch 23/30\n",
      "551/551 [==============================] - 830s 2s/step - loss: 0.4527 - accuracy: 0.8320 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 24/30\n",
      "551/551 [==============================] - 831s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4454 - val_accuracy: 0.8365\n",
      "Epoch 25/30\n",
      "551/551 [==============================] - 829s 2s/step - loss: 0.4528 - accuracy: 0.8320 - val_loss: 0.4467 - val_accuracy: 0.8357\n",
      "Epoch 26/30\n",
      "551/551 [==============================] - 831s 2s/step - loss: 0.4530 - accuracy: 0.8318 - val_loss: 0.4442 - val_accuracy: 0.8373\n",
      "Epoch 27/30\n",
      "551/551 [==============================] - 830s 2s/step - loss: 0.4527 - accuracy: 0.8321 - val_loss: 0.4455 - val_accuracy: 0.8365\n",
      "Epoch 28/30\n",
      "551/551 [==============================] - 832s 2s/step - loss: 0.4527 - accuracy: 0.8321 - val_loss: 0.4455 - val_accuracy: 0.8365\n",
      "Epoch 29/30\n",
      "551/551 [==============================] - 831s 2s/step - loss: 0.4527 - accuracy: 0.8320 - val_loss: 0.4480 - val_accuracy: 0.8349\n",
      "Epoch 30/30\n",
      "551/551 [==============================] - 831s 2s/step - loss: 0.4526 - accuracy: 0.8321 - val_loss: 0.4454 - val_accuracy: 0.8365\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Definir o caminho para as pastas de treinamento e validação\n",
    "train_data_dir = 'train'\n",
    "valid_data_dir = 'validation'\n",
    "\n",
    "# Definir o número de classes\n",
    "num_classes = 2\n",
    "\n",
    "# Definir o tamanho da imagem de entrada\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "# Criar uma instância da ResNet50 pré-treinada\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "# Congelar os pesos da ResNet50 para que não sejam atualizados durante o treinamento\n",
    "base_model.trainable = False\n",
    "\n",
    "# Criar o modelo sequencial\n",
    "model = Sequential()\n",
    "\n",
    "# Adicionar a ResNet50 como base\n",
    "model.add(base_model)\n",
    "\n",
    "# Adicionar camadas adicionais para a rede profunda\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Definir os hiperparâmetros do treinamento\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Pré-processamento dos dados de treinamento e validação\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_data_dir,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Treinamento do modelo\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,  \n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "model.save('resnet_modelFinal.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo gasto na versão com CPU (2 threads): 3.3800973296165466 minutos\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from concurrent import futures\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "\n",
    "def aumentarDados(imagem):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        iaa.GammaContrast((0.8, 1.2)),\n",
    "        iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25),\n",
    "        iaa.Affine(scale=(0.8, 1.2))\n",
    "    ])\n",
    "\n",
    "    imagens_aumentadas = seq.augment_images([imagem] * 5)  # Gerar 5 imagens aumentadas\n",
    "\n",
    "    return imagens_aumentadas\n",
    "\n",
    "\n",
    "def processar_imagem(arquivo, pasta_origem, pasta_destino):\n",
    "    caminho_origem = os.path.join(pasta_origem, arquivo)\n",
    "    imagem = cv2.imread(caminho_origem)\n",
    "    imagens_aumentadas = aumentarDados(imagem)\n",
    "    for i, img_aumentada in enumerate(imagens_aumentadas):\n",
    "        nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "        caminho_destino = os.path.join(pasta_destino, nome_arquivo)\n",
    "        cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "\n",
    "def aplicar_aumento_dados(pasta_train):\n",
    "    pasta_destino_aumentadas_0 = os.path.join(pasta_train, \"0\")\n",
    "    pasta_destino_aumentadas_1 = os.path.join(pasta_train, \"1\")\n",
    "\n",
    "    # Criar os diretórios de destino se não existirem\n",
    "    if not os.path.exists(pasta_destino_aumentadas_0):\n",
    "        os.makedirs(pasta_destino_aumentadas_0)\n",
    "    if not os.path.exists(pasta_destino_aumentadas_1):\n",
    "        os.makedirs(pasta_destino_aumentadas_1)\n",
    "\n",
    "    # Percorrer a pasta \"0\" e aplicar aumento de dados nas imagens\n",
    "    pasta_origem_0 = os.path.join(pasta_train, \"0\")\n",
    "    imagens_0 = [arquivo for arquivo in os.listdir(pasta_origem_0) if arquivo.endswith(\".png\")]\n",
    "    with futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        for arquivo in imagens_0:\n",
    "            executor.submit(processar_imagem, arquivo, pasta_origem_0, pasta_destino_aumentadas_0)\n",
    "\n",
    "    # Percorrer a pasta \"1\" e aplicar aumento de dados nas imagens\n",
    "    pasta_origem_1 = os.path.join(pasta_train, \"1\")\n",
    "    imagens_1 = [arquivo for arquivo in os.listdir(pasta_origem_1) if arquivo.endswith(\".png\")]\n",
    "    with futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        for arquivo in imagens_1:\n",
    "            executor.submit(processar_imagem, arquivo, pasta_origem_1, pasta_destino_aumentadas_1)\n",
    "\n",
    "\n",
    "pasta_train = \"train2\"\n",
    "start_time = time.time()\n",
    "aplicar_aumento_dados(pasta_train)\n",
    "cpu_time = time.time() - start_time\n",
    "\n",
    "tempo_minutos = cpu_time / 60\n",
    "print(\"Tempo gasto na versão com CPU (2 threads):\", tempo_minutos, \"minutos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m pasta_train \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 56\u001b[0m aplicar_aumento_dados(pasta_train)\n\u001b[0;32m     57\u001b[0m cpu_time \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time) \u001b[39m/\u001b[39m \u001b[39m60\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTempo gasto na versão CPU:\u001b[39m\u001b[39m\"\u001b[39m, cpu_time, \u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[47], line 40\u001b[0m, in \u001b[0;36maplicar_aumento_dados\u001b[1;34m(pasta_train)\u001b[0m\n\u001b[0;32m     38\u001b[0m             nome_arquivo \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maumentada_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00marquivo\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m             caminho_destino \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pasta_destino_aumentadas_0, nome_arquivo)\n\u001b[1;32m---> 40\u001b[0m             cv2\u001b[39m.\u001b[39;49mimwrite(caminho_destino, img_aumentada)\n\u001b[0;32m     42\u001b[0m \u001b[39m# Process the \"1\" folder and apply data augmentation to the images\u001b[39;00m\n\u001b[0;32m     43\u001b[0m pasta_origem_1 \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pasta_train, \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "def aumentarDados(imagem):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.GaussianBlur(sigma=(0, 1.0)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        iaa.GammaContrast((0.8, 1.2)),\n",
    "        iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25),\n",
    "        iaa.Affine(scale=(0.8, 1.2))\n",
    "    ])\n",
    "\n",
    "    imagens_aumentadas = seq.augment_images([imagem] * 5)  # Generate 5 augmented images\n",
    "\n",
    "    return imagens_aumentadas\n",
    "\n",
    "def aplicar_aumento_dados(pasta_train):\n",
    "    pasta_destino_aumentadas_0 = os.path.join(pasta_train, \"0\")\n",
    "    pasta_destino_aumentadas_1 = os.path.join(pasta_train, \"1\")\n",
    "\n",
    "    # Create destination directories if they don't exist\n",
    "    if not os.path.exists(pasta_destino_aumentadas_0):\n",
    "        os.makedirs(pasta_destino_aumentadas_0)\n",
    "    if not os.path.exists(pasta_destino_aumentadas_1):\n",
    "        os.makedirs(pasta_destino_aumentadas_1)\n",
    "\n",
    "    # Process the \"0\" folder and apply data augmentation to the images\n",
    "    pasta_origem_0 = os.path.join(pasta_train, \"0\")\n",
    "    for arquivo in os.listdir(pasta_origem_0):\n",
    "        if arquivo.endswith(\".png\"):\n",
    "            caminho_origem = os.path.join(pasta_origem_0, arquivo)\n",
    "            imagem = cv2.imread(caminho_origem)\n",
    "            imagens_aumentadas = aumentarDados(imagem)\n",
    "            for i, img_aumentada in enumerate(imagens_aumentadas):\n",
    "                nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "                caminho_destino = os.path.join(pasta_destino_aumentadas_0, nome_arquivo)\n",
    "                cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "    # Process the \"1\" folder and apply data augmentation to the images\n",
    "    pasta_origem_1 = os.path.join(pasta_train, \"1\")\n",
    "    for arquivo in os.listdir(pasta_origem_1):\n",
    "        if arquivo.endswith(\".png\"):\n",
    "            caminho_origem = os.path.join(pasta_origem_1, arquivo)\n",
    "            imagem = cv2.imread(caminho_origem)\n",
    "            imagens_aumentadas = aumentarDados(imagem)\n",
    "            for i, img_aumentada in enumerate(imagens_aumentadas):\n",
    "                nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "                caminho_destino = os.path.join(pasta_destino_aumentadas_1, nome_arquivo)\n",
    "                cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "pasta_train = \"train2\"\n",
    "start_time = time.time()\n",
    "aplicar_aumento_dados(pasta_train)\n",
    "cpu_time = (time.time() - start_time) / 60\n",
    "\n",
    "print(\"Tempo gasto na versão CPU:\", cpu_time, \"min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "BATCH_SIZE       = 32\n",
    "IMG_HEIGHT_WIDTH = 256\n",
    "IMG_INPUT_SHAPE  = (IMG_HEIGHT_WIDTH, IMG_HEIGHT_WIDTH, 3)\n",
    "MAX_EPOCHS       = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'train'\n",
    "valid_data_dir = 'validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"TB_Chest_Radiography_Database/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4200 files belonging to 3 classes.\n",
      "Using 3360 files for training.\n",
      "Found 4200 files belonging to 3 classes.\n",
      "Using 840 files for validation.\n"
     ]
    }
   ],
   "source": [
    "DS_TRAIN = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_HEIGHT_WIDTH, IMG_HEIGHT_WIDTH),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "DS_VALID = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_HEIGHT_WIDTH, IMG_HEIGHT_WIDTH),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "DS_TRAIN = DS_TRAIN.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "DS_VALID = DS_VALID.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(tf_model):\n",
    "    model = None\n",
    "    if tf_model == \"VGG16\":\n",
    "        model = tf.keras.applications.VGG16(include_top=False, weights=\"imagenet\", input_tensor = tf.keras.Input(shape=IMG_INPUT_SHAPE))\n",
    "    elif tf_model == \"ResNet50\":\n",
    "        model = tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\", input_tensor = tf.keras.Input(shape=IMG_INPUT_SHAPE))\n",
    "    elif tf_model == \"MobileNetV2\":\n",
    "        model = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_tensor = tf.keras.Input(shape=IMG_INPUT_SHAPE))\n",
    "    else:\n",
    "        raise Exception('SUNXYZ', 'Model unknown')\n",
    "    return_model = tf.keras.models.Sequential()\n",
    "    return_model.add(model)\n",
    "    return_model.add(tf.keras.layers.Flatten())\n",
    "    return_model.add(tf.keras.layers.Dense(512))\n",
    "    return_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    return_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    return_model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "    return return_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_MODEL = tf.keras.models.Sequential()\n",
    "BASIC_MODEL.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "BASIC_MODEL.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "BASIC_MODEL.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "BASIC_MODEL.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "BASIC_MODEL.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "BASIC_MODEL.add(tf.keras.layers.Flatten())\n",
    "BASIC_MODEL.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "BASIC_MODEL.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "BASIC_MODEL.add(tf.keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESNET50_MODEL    = get_model(\"ResNet50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 131072)            0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 512)               67109376  \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,844,994\n",
      "Trainable params: 90,791,874\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RESNET50_MODEL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model):\n",
    "    model.compile(optimizer='adam',\n",
    "      loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "      DS_TRAIN,\n",
    "      validation_data=DS_VALID,\n",
    "      epochs=MAX_EPOCHS\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "105/105 [==============================] - 612s 6s/step - loss: 6.7602 - accuracy: 0.8958 - val_loss: 1270.3171 - val_accuracy: 0.2869\n",
      "Epoch 2/30\n",
      "105/105 [==============================] - 600s 6s/step - loss: 0.3977 - accuracy: 0.9440 - val_loss: 2.8805 - val_accuracy: 0.8738\n",
      "Epoch 3/30\n",
      "105/105 [==============================] - 603s 6s/step - loss: 0.1811 - accuracy: 0.9676 - val_loss: 0.1521 - val_accuracy: 0.9536\n",
      "Epoch 4/30\n",
      "105/105 [==============================] - 600s 6s/step - loss: 0.1692 - accuracy: 0.9717 - val_loss: 141.3881 - val_accuracy: 0.5595\n",
      "Epoch 5/30\n",
      "105/105 [==============================] - 603s 6s/step - loss: 0.6577 - accuracy: 0.9548 - val_loss: 46.9991 - val_accuracy: 0.8857\n",
      "Epoch 6/30\n",
      "105/105 [==============================] - 601s 6s/step - loss: 0.6797 - accuracy: 0.9315 - val_loss: 1.5741 - val_accuracy: 0.9012\n",
      "Epoch 7/30\n",
      "105/105 [==============================] - 602s 6s/step - loss: 0.1958 - accuracy: 0.9452 - val_loss: 0.4496 - val_accuracy: 0.8940\n",
      "Epoch 8/30\n",
      "105/105 [==============================] - 621s 6s/step - loss: 0.1094 - accuracy: 0.9613 - val_loss: 0.1616 - val_accuracy: 0.9476\n",
      "Epoch 9/30\n",
      "105/105 [==============================] - 590s 6s/step - loss: 0.0840 - accuracy: 0.9682 - val_loss: 0.1288 - val_accuracy: 0.9571\n",
      "Epoch 10/30\n",
      "105/105 [==============================] - 585s 6s/step - loss: 0.0651 - accuracy: 0.9744 - val_loss: 0.0821 - val_accuracy: 0.9702\n",
      "Epoch 11/30\n",
      "105/105 [==============================] - 623s 6s/step - loss: 0.0515 - accuracy: 0.9804 - val_loss: 0.0940 - val_accuracy: 0.9702\n",
      "Epoch 12/30\n",
      "105/105 [==============================] - 603s 6s/step - loss: 0.1218 - accuracy: 0.9702 - val_loss: 1.1678 - val_accuracy: 0.5905\n",
      "Epoch 13/30\n",
      "105/105 [==============================] - 605s 6s/step - loss: 0.0725 - accuracy: 0.9765 - val_loss: 0.4214 - val_accuracy: 0.8048\n",
      "Epoch 14/30\n",
      "105/105 [==============================] - 600s 6s/step - loss: 0.0494 - accuracy: 0.9810 - val_loss: 0.0711 - val_accuracy: 0.9774\n",
      "Epoch 15/30\n",
      "105/105 [==============================] - 585s 6s/step - loss: 0.0478 - accuracy: 0.9854 - val_loss: 0.0732 - val_accuracy: 0.9762\n",
      "Epoch 16/30\n",
      "105/105 [==============================] - 600s 6s/step - loss: 0.0327 - accuracy: 0.9899 - val_loss: 0.0702 - val_accuracy: 0.9702\n",
      "Epoch 17/30\n",
      "105/105 [==============================] - 598s 6s/step - loss: 0.0253 - accuracy: 0.9920 - val_loss: 0.2881 - val_accuracy: 0.8762\n",
      "Epoch 18/30\n",
      "105/105 [==============================] - 596s 6s/step - loss: 0.0214 - accuracy: 0.9914 - val_loss: 0.0506 - val_accuracy: 0.9821\n",
      "Epoch 19/30\n",
      "105/105 [==============================] - 595s 6s/step - loss: 0.0241 - accuracy: 0.9917 - val_loss: 0.1420 - val_accuracy: 0.9702\n",
      "Epoch 20/30\n",
      "105/105 [==============================] - 596s 6s/step - loss: 0.0184 - accuracy: 0.9949 - val_loss: 0.0476 - val_accuracy: 0.9821\n",
      "Epoch 21/30\n",
      "105/105 [==============================] - 598s 6s/step - loss: 0.0126 - accuracy: 0.9961 - val_loss: 0.2531 - val_accuracy: 0.9167\n",
      "Epoch 22/30\n",
      "105/105 [==============================] - 599s 6s/step - loss: 0.0123 - accuracy: 0.9967 - val_loss: 0.0994 - val_accuracy: 0.9774\n",
      "Epoch 23/30\n",
      "105/105 [==============================] - 598s 6s/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0578 - val_accuracy: 0.9845\n",
      "Epoch 24/30\n",
      "105/105 [==============================] - 597s 6s/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.0953 - val_accuracy: 0.9845\n",
      "Epoch 25/30\n",
      "105/105 [==============================] - 609s 6s/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.1031 - val_accuracy: 0.9810\n",
      "Epoch 26/30\n",
      "105/105 [==============================] - 611s 6s/step - loss: 0.0114 - accuracy: 0.9973 - val_loss: 0.1530 - val_accuracy: 0.9595\n",
      "Epoch 27/30\n",
      "105/105 [==============================] - 610s 6s/step - loss: 0.1116 - accuracy: 0.9699 - val_loss: 1.6949 - val_accuracy: 0.5726\n",
      "Epoch 28/30\n",
      "105/105 [==============================] - 608s 6s/step - loss: 0.1352 - accuracy: 0.9765 - val_loss: 1.0673 - val_accuracy: 0.5345\n",
      "Epoch 29/30\n",
      "105/105 [==============================] - 603s 6s/step - loss: 0.1379 - accuracy: 0.9604 - val_loss: 30.6313 - val_accuracy: 0.9107\n",
      "Epoch 30/30\n",
      "105/105 [==============================] - 610s 6s/step - loss: 0.2851 - accuracy: 0.9640 - val_loss: 0.3180 - val_accuracy: 0.9155\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m RESNET50_HISTORY \u001b[39m=\u001b[39m compile_and_fit(RESNET50_MODEL)\n\u001b[1;32m----> 2\u001b[0m RESNET50_HISTORY\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39m\u001b[39mfinal.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "RESNET50_HISTORY = compile_and_fit(RESNET50_MODEL)\n",
    "RESNET50_HISTORY.save('final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESNET50_MODEL.save('final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 622ms/step\n",
      "[[6.570386e-05 9.999343e-01]]\n",
      "Não existe com a probabilidade de 0.9999343\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def ttt():\n",
    "    img_path = \"imagem.png\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Converte para escala de cinza\n",
    "\n",
    "    # Aplica a equalização do histograma\n",
    "    eg = cv2.equalizeHist(gray)\n",
    "    eg_rgb = cv2.cvtColor(eg, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Carrega o modelo pré-treinado\n",
    "    model = tf.keras.models.load_model(\"final.h5\")\n",
    "\n",
    "    # Realiza a predição\n",
    "    prediction = model.predict(np.expand_dims(eg_rgb, axis=0))\n",
    "    print(prediction)\n",
    "    if prediction[0][0] < prediction[0][1]:\n",
    "        print(\"Existe com a probabilidade de\", prediction[0][0])\n",
    "    else:\n",
    "        print(\"Não existe com a probabilidade de\", prediction[0][1])\n",
    "\n",
    "ttt()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NvvmSupportError",
     "evalue": "libNVVM cannot be found. Do `conda install cudatoolkit`:\n[WinError 3] O sistema não pode encontrar o caminho especificado: 'c:\\\\Users\\\\felipe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Library\\\\bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\cudadrv\\nvvm.py:139\u001b[0m, in \u001b[0;36mNVVM.__new__\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     inst\u001b[39m.\u001b[39mdriver \u001b[39m=\u001b[39m open_cudalib(\u001b[39m'\u001b[39;49m\u001b[39mnvvm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\cudadrv\\libs.py:63\u001b[0m, in \u001b[0;36mopen_cudalib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen_cudalib\u001b[39m(lib):\n\u001b[1;32m---> 63\u001b[0m     path \u001b[39m=\u001b[39m get_cudalib(lib)\n\u001b[0;32m     64\u001b[0m     \u001b[39mreturn\u001b[39;00m ctypes\u001b[39m.\u001b[39mCDLL(path)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\cudadrv\\libs.py:52\u001b[0m, in \u001b[0;36mget_cudalib\u001b[1;34m(lib, platform, static)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m lib \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnvvm\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[39mreturn\u001b[39;00m get_cuda_paths()[\u001b[39m'\u001b[39m\u001b[39mnvvm\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39minfo \u001b[39mor\u001b[39;00m _dllnamepattern \u001b[39m%\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnvvm\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\cuda_paths.py:223\u001b[0m, in \u001b[0;36mget_cuda_paths\u001b[1;34m()\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[39m# Not in cache\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     d \u001b[39m=\u001b[39m {\n\u001b[1;32m--> 223\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnvvm\u001b[39m\u001b[39m'\u001b[39m: _get_nvvm_path(),\n\u001b[0;32m    224\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlibdevice\u001b[39m\u001b[39m'\u001b[39m: _get_libdevice_paths(),\n\u001b[0;32m    225\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcudalib_dir\u001b[39m\u001b[39m'\u001b[39m: _get_cudalib_dir(),\n\u001b[0;32m    226\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstatic_cudalib_dir\u001b[39m\u001b[39m'\u001b[39m: _get_static_cudalib_dir(),\n\u001b[0;32m    227\u001b[0m     }\n\u001b[0;32m    228\u001b[0m     \u001b[39m# Cache result\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\cuda_paths.py:201\u001b[0m, in \u001b[0;36m_get_nvvm_path\u001b[1;34m()\u001b[0m\n\u001b[0;32m    200\u001b[0m by, path \u001b[39m=\u001b[39m _get_nvvm_path_decision()\n\u001b[1;32m--> 201\u001b[0m candidates \u001b[39m=\u001b[39m find_lib(\u001b[39m'\u001b[39;49m\u001b[39mnvvm\u001b[39;49m\u001b[39m'\u001b[39;49m, path)\n\u001b[0;32m    202\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(candidates) \u001b[39mif\u001b[39;00m candidates \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\misc\\findlib.py:44\u001b[0m, in \u001b[0;36mfind_lib\u001b[1;34m(libname, libdir, platform, static)\u001b[0m\n\u001b[0;32m     43\u001b[0m regex \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(pat)\n\u001b[1;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m find_file(regex, libdir)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\misc\\findlib.py:56\u001b[0m, in \u001b[0;36mfind_file\u001b[1;34m(pat, libdir)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m ldir \u001b[39min\u001b[39;00m libdirs:\n\u001b[1;32m---> 56\u001b[0m     entries \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(ldir)\n\u001b[0;32m     57\u001b[0m     candidates \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ldir, ent)\n\u001b[0;32m     58\u001b[0m                   \u001b[39mfor\u001b[39;00m ent \u001b[39min\u001b[39;00m entries \u001b[39mif\u001b[39;00m pat\u001b[39m.\u001b[39mmatch(ent)]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] O sistema não pode encontrar o caminho especificado: 'c:\\\\Users\\\\felipe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Library\\\\bin'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNvvmSupportError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m cuda, vectorize\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimgaug\u001b[39;00m \u001b[39mimport\u001b[39;00m augmenters \u001b[39mas\u001b[39;00m iaa\n\u001b[1;32m----> 8\u001b[0m \u001b[39m@vectorize\u001b[39;49m([\u001b[39m'\u001b[39;49m\u001b[39muint8(uint8, uint8)\u001b[39;49m\u001b[39m'\u001b[39;49m], target\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39maugment_pixel\u001b[39;49m(original_pixel, random_pixel):\n\u001b[0;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;49;00m original_pixel \u001b[39m+\u001b[39;49m random_pixel\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maumentarDados\u001b[39m(imagem, imagens_aumentadas):\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\np\\ufunc\\decorators.py:131\u001b[0m, in \u001b[0;36mvectorize.<locals>.wrap\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    129\u001b[0m vec \u001b[39m=\u001b[39m Vectorize(func, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws)\n\u001b[0;32m    130\u001b[0m \u001b[39mfor\u001b[39;00m sig \u001b[39min\u001b[39;00m ftylist:\n\u001b[1;32m--> 131\u001b[0m     vec\u001b[39m.\u001b[39;49madd(sig)\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ftylist) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    133\u001b[0m     vec\u001b[39m.\u001b[39mdisable_compile()\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\np\\ufunc\\deviceufunc.py:388\u001b[0m, in \u001b[0;36mDeviceVectorize.add\u001b[1;34m(self, sig)\u001b[0m\n\u001b[0;32m    385\u001b[0m funcname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m    386\u001b[0m kernelsource \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_kernel_source(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kernel_template,\n\u001b[0;32m    387\u001b[0m                                        devfnsig, funcname)\n\u001b[1;32m--> 388\u001b[0m corefn, return_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compile_core(devfnsig)\n\u001b[0;32m    389\u001b[0m glbl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_globals(corefn)\n\u001b[0;32m    390\u001b[0m sig \u001b[39m=\u001b[39m signature(types\u001b[39m.\u001b[39mvoid, \u001b[39m*\u001b[39m([a[:] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args] \u001b[39m+\u001b[39m [return_type[:]]))\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\vectorizers.py:202\u001b[0m, in \u001b[0;36mCUDAVectorize._compile_core\u001b[1;34m(self, sig)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compile_core\u001b[39m(\u001b[39mself\u001b[39m, sig):\n\u001b[1;32m--> 202\u001b[0m     cudevfn \u001b[39m=\u001b[39m cuda\u001b[39m.\u001b[39;49mjit(sig, device\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, inline\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpyfunc)\n\u001b[0;32m    203\u001b[0m     \u001b[39mreturn\u001b[39;00m cudevfn, cudevfn\u001b[39m.\u001b[39moverloads[sig\u001b[39m.\u001b[39margs]\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mreturn_type\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\decorators.py:131\u001b[0m, in \u001b[0;36mjit.<locals>._jit\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m typeinfer\n\u001b[0;32m    130\u001b[0m     \u001b[39mwith\u001b[39;00m typeinfer\u001b[39m.\u001b[39mregister_dispatcher(disp):\n\u001b[1;32m--> 131\u001b[0m         disp\u001b[39m.\u001b[39;49mcompile_device(argtypes, restype)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     disp\u001b[39m.\u001b[39mcompile(argtypes)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:878\u001b[0m, in \u001b[0;36mCUDADispatcher.compile_device\u001b[1;34m(self, args, return_type)\u001b[0m\n\u001b[0;32m    872\u001b[0m nvvm_options \u001b[39m=\u001b[39m {\n\u001b[0;32m    873\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mopt\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargetoptions\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mopt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m,\n\u001b[0;32m    874\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfastmath\u001b[39m\u001b[39m'\u001b[39m: fastmath\n\u001b[0;32m    875\u001b[0m }\n\u001b[0;32m    877\u001b[0m cc \u001b[39m=\u001b[39m get_current_device()\u001b[39m.\u001b[39mcompute_capability\n\u001b[1;32m--> 878\u001b[0m cres \u001b[39m=\u001b[39m compile_cuda(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpy_func, return_type, args,\n\u001b[0;32m    879\u001b[0m                     debug\u001b[39m=\u001b[39;49mdebug,\n\u001b[0;32m    880\u001b[0m                     lineinfo\u001b[39m=\u001b[39;49mlineinfo,\n\u001b[0;32m    881\u001b[0m                     inline\u001b[39m=\u001b[39;49minline,\n\u001b[0;32m    882\u001b[0m                     fastmath\u001b[39m=\u001b[39;49mfastmath,\n\u001b[0;32m    883\u001b[0m                     nvvm_options\u001b[39m=\u001b[39;49mnvvm_options,\n\u001b[0;32m    884\u001b[0m                     cc\u001b[39m=\u001b[39;49mcc)\n\u001b[0;32m    885\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverloads[args] \u001b[39m=\u001b[39m cres\n\u001b[0;32m    887\u001b[0m cres\u001b[39m.\u001b[39mtarget_context\u001b[39m.\u001b[39minsert_user_function(cres\u001b[39m.\u001b[39mentry_point,\n\u001b[0;32m    888\u001b[0m                                          cres\u001b[39m.\u001b[39mfndesc,\n\u001b[0;32m    889\u001b[0m                                          [cres\u001b[39m.\u001b[39mlibrary])\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_acquire_compile_lock\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\compiler.py:230\u001b[0m, in \u001b[0;36mcompile_cuda\u001b[1;34m(pyfunc, return_type, args, debug, lineinfo, inline, fastmath, nvvm_options, cc)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtarget_extension\u001b[39;00m \u001b[39mimport\u001b[39;00m target_override\n\u001b[0;32m    229\u001b[0m \u001b[39mwith\u001b[39;00m target_override(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 230\u001b[0m     cres \u001b[39m=\u001b[39m compiler\u001b[39m.\u001b[39;49mcompile_extra(typingctx\u001b[39m=\u001b[39;49mtypingctx,\n\u001b[0;32m    231\u001b[0m                                   targetctx\u001b[39m=\u001b[39;49mtargetctx,\n\u001b[0;32m    232\u001b[0m                                   func\u001b[39m=\u001b[39;49mpyfunc,\n\u001b[0;32m    233\u001b[0m                                   args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    234\u001b[0m                                   return_type\u001b[39m=\u001b[39;49mreturn_type,\n\u001b[0;32m    235\u001b[0m                                   flags\u001b[39m=\u001b[39;49mflags,\n\u001b[0;32m    236\u001b[0m                                   \u001b[39mlocals\u001b[39;49m\u001b[39m=\u001b[39;49m{},\n\u001b[0;32m    237\u001b[0m                                   pipeline_class\u001b[39m=\u001b[39;49mCUDACompiler)\n\u001b[0;32m    239\u001b[0m library \u001b[39m=\u001b[39m cres\u001b[39m.\u001b[39mlibrary\n\u001b[0;32m    240\u001b[0m library\u001b[39m.\u001b[39mfinalize()\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler.py:742\u001b[0m, in \u001b[0;36mcompile_extra\u001b[1;34m(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compiler entry point\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \n\u001b[0;32m    720\u001b[0m \u001b[39mParameter\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[39m    compiler pipeline\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    740\u001b[0m pipeline \u001b[39m=\u001b[39m pipeline_class(typingctx, targetctx, library,\n\u001b[0;32m    741\u001b[0m                           args, return_type, flags, \u001b[39mlocals\u001b[39m)\n\u001b[1;32m--> 742\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline\u001b[39m.\u001b[39;49mcompile_extra(func)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler.py:460\u001b[0m, in \u001b[0;36mCompilerBase.compile_extra\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mlifted \u001b[39m=\u001b[39m ()\n\u001b[0;32m    459\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mlifted_from \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compile_bytecode()\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler.py:528\u001b[0m, in \u001b[0;36mCompilerBase._compile_bytecode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39mPopulate and run pipeline for bytecode input\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfunc_ir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 528\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compile_core()\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler.py:507\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus\u001b[39m.\u001b[39mfail_reason \u001b[39m=\u001b[39m e\n\u001b[0;32m    506\u001b[0m         \u001b[39mif\u001b[39;00m is_final_pipeline:\n\u001b[1;32m--> 507\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    508\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m CompilerError(\u001b[39m\"\u001b[39m\u001b[39mAll available pipelines exhausted\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler.py:494\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 494\u001b[0m     pm\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mcr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_machinery.py:368\u001b[0m, in \u001b[0;36mPassManager.run\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    365\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFailed in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m mode pipeline (step: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \\\n\u001b[0;32m    366\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline_name, pass_desc)\n\u001b[0;32m    367\u001b[0m patched_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_patch_error(msg, e)\n\u001b[1;32m--> 368\u001b[0m \u001b[39mraise\u001b[39;00m patched_exception\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_machinery.py:356\u001b[0m, in \u001b[0;36mPassManager.run\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    354\u001b[0m pass_inst \u001b[39m=\u001b[39m _pass_registry\u001b[39m.\u001b[39mget(pss)\u001b[39m.\u001b[39mpass_inst\n\u001b[0;32m    355\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(pass_inst, CompilerPass):\n\u001b[1;32m--> 356\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_runPass(idx, pass_inst, state)\n\u001b[0;32m    357\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mBaseException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLegacy pass in use\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_acquire_compile_lock\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_machinery.py:311\u001b[0m, in \u001b[0;36mPassManager._runPass\u001b[1;34m(self, index, pss, internal_state)\u001b[0m\n\u001b[0;32m    309\u001b[0m     mutated \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m check(pss\u001b[39m.\u001b[39mrun_initialization, internal_state)\n\u001b[0;32m    310\u001b[0m \u001b[39mwith\u001b[39;00m SimpleTimer() \u001b[39mas\u001b[39;00m pass_time:\n\u001b[1;32m--> 311\u001b[0m     mutated \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m check(pss\u001b[39m.\u001b[39;49mrun_pass, internal_state)\n\u001b[0;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m SimpleTimer() \u001b[39mas\u001b[39;00m finalize_time:\n\u001b[0;32m    313\u001b[0m     mutated \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m check(pss\u001b[39m.\u001b[39mrun_finalizer, internal_state)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\core\\compiler_machinery.py:273\u001b[0m, in \u001b[0;36mPassManager._runPass.<locals>.check\u001b[1;34m(func, compiler_state)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck\u001b[39m(func, compiler_state):\n\u001b[1;32m--> 273\u001b[0m     mangled \u001b[39m=\u001b[39m func(compiler_state)\n\u001b[0;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m mangled \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    275\u001b[0m         msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mCompilerPass implementations should return True/False. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m                \u001b[39m\"\u001b[39m\u001b[39mCompilerPass with name \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m did not.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\compiler.py:128\u001b[0m, in \u001b[0;36mCUDALegalization.run_pass\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_pass\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m    126\u001b[0m     \u001b[39m# Early return if NVVM 7\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcuda\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcudadrv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnvvm\u001b[39;00m \u001b[39mimport\u001b[39;00m NVVM\n\u001b[1;32m--> 128\u001b[0m     \u001b[39mif\u001b[39;00m NVVM()\u001b[39m.\u001b[39mis_nvvm70:\n\u001b[0;32m    129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     \u001b[39m# NVVM < 7, need to check for charseq\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\felipe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numba\\cuda\\cudadrv\\nvvm.py:144\u001b[0m, in \u001b[0;36mNVVM.__new__\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__INSTANCE \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     errmsg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mlibNVVM cannot be found. Do `conda install \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mcudatoolkit`:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mraise\u001b[39;00m NvvmSupportError(errmsg \u001b[39m%\u001b[39m e)\n\u001b[0;32m    146\u001b[0m \u001b[39m# Find & populate functions\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m name, proto \u001b[39min\u001b[39;00m inst\u001b[39m.\u001b[39m_PROTOTYPES\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mNvvmSupportError\u001b[0m: libNVVM cannot be found. Do `conda install cudatoolkit`:\n[WinError 3] O sistema não pode encontrar o caminho especificado: 'c:\\\\Users\\\\felipe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Library\\\\bin'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from numba import cuda, vectorize\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "@vectorize(['uint8(uint8, uint8)'], target='cuda')\n",
    "def augment_pixel(original_pixel, random_pixel):\n",
    "    return original_pixel + random_pixel\n",
    "\n",
    "def aumentarDados(imagem, imagens_aumentadas):\n",
    "    random_pixels = np.random.randint(0, 13, size=imagens_aumentadas.shape, dtype=np.uint8)\n",
    "    imagens_aumentadas[:] = augment_pixel(imagem, random_pixels)\n",
    "\n",
    "def aplicar_aumento_dados(pasta_train):\n",
    "    pasta_destino_aumentadas_0 = os.path.join(pasta_train, \"0\")\n",
    "    pasta_destino_aumentadas_1 = os.path.join(pasta_train, \"1\")\n",
    "\n",
    "    # Create destination directories if they don't exist\n",
    "    if not os.path.exists(pasta_destino_aumentadas_0):\n",
    "        os.makedirs(pasta_destino_aumentadas_0)\n",
    "    if not os.path.exists(pasta_destino_aumentadas_1):\n",
    "        os.makedirs(pasta_destino_aumentadas_1)\n",
    "\n",
    "    # Process the \"0\" folder and apply data augmentation to the images\n",
    "    pasta_origem_0 = os.path.join(pasta_train, \"0\")\n",
    "    for arquivo in os.listdir(pasta_origem_0):\n",
    "        if arquivo.endswith(\".png\"):\n",
    "            caminho_origem = os.path.join(pasta_origem_0, arquivo)\n",
    "            imagem = cv2.imread(caminho_origem)\n",
    "            imagens_aumentadas = np.empty((5,) + imagem.shape, dtype=np.uint8)\n",
    "            aumentarDados(imagem, imagens_aumentadas)\n",
    "            for i, img_aumentada in enumerate(imagens_aumentadas):\n",
    "                nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "                caminho_destino = os.path.join(pasta_destino_aumentadas_0, nome_arquivo)\n",
    "                cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "    # Process the \"1\" folder and apply data augmentation to the images\n",
    "    pasta_origem_1 = os.path.join(pasta_train, \"1\")\n",
    "    for arquivo in os.listdir(pasta_origem_1):\n",
    "        if arquivo.endswith(\".png\"):\n",
    "            caminho_origem = os.path.join(pasta_origem_1, arquivo)\n",
    "            imagem = cv2.imread(caminho_origem)\n",
    "            imagens_aumentadas = np.empty((5,) + imagem.shape, dtype=np.uint8)\n",
    "            aumentarDados(imagem, imagens_aumentadas)\n",
    "            for i, img_aumentada in enumerate(imagens_aumentadas):\n",
    "                nome_arquivo = f\"aumentada_{i}_{arquivo}\"\n",
    "                caminho_destino = os.path.join(pasta_destino_aumentadas_1, nome_arquivo)\n",
    "                cv2.imwrite(caminho_destino, img_aumentada)\n",
    "\n",
    "pasta_train = \"train2\"\n",
    "start_time = time.time()\n",
    "aplicar_aumento_dados(pasta_train)\n",
    "gpu_time = (time.time() - start_time) / 60\n",
    "\n",
    "print(\"Tempo gasto na versão GPU:\", gpu_time, \"min\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
